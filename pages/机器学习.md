- 机器学习（Machine Learning）指从有限的观测数据中学习出一般的规律，并利用这些规律对未知数据进行预测的方法。
- ## 学习资料
- 课程资源：
	- CS229 机器学习：[Stanford CS229 bilibili 视频](https://www.bilibili.com/video/BV1JE411w7Ub/)、[课程讲义](https://www.123pan.com/s/plj7Vv-iH223.html)、[中文笔记](https://doraemonzzz.com/tags/CS229/)
	- QianXiao Li 暑期学校：[讲义](https://www.123pan.com/s/plj7Vv-Dm223.html)
- 资料：[神经网络与深度神经网络](https://www.123pan.com/s/plj7Vv-tH223.html)
  id:: 665e85bd-62d4-4665-965c-27f9ffbf5a65
-
- ## 机器学习三要素
- 机器学习任务首先要确定输入空间 $\mathcal{X}$ 和输出空间 $\mathcal{Y}$，并建立 $x \in \mathcal{X}, y \in \mathcal{Y}$ 之间的关系。不妨设 $x,y$ 真实关系为 $y = g(x)$ 或条件概率分布 $p(y|x)$ ，或者最普遍的有 $(x,y)$ 服从分布 $p(x,y)$。
- 一般设数据集 $\mathcal{D} = \{{x}^{(n)}, y^{(n)}\}_{n = 1}^M$ 是独立同分布的，即每个 $({x}^{(i)}, y^{(i)})$ 都是从未知分布 $p({x},y)$ 中随机产生的。
-
- 模型（即 ${x}$ 与 $y$ 的关系假设）：由于不知道 $g({x})$ 或 $p(y|{x})$ 的具体形式，只能根据经验来假设一个函数集合 $\mathcal{H}$，称为假设空间，假设空间通常为一个参数化的函数族
  logseq.order-list-type:: number
  
  $$ \mathcal{H} = \{h({x};\theta)| \theta \in \mathbb{R}^D\}, $$
  
  其中 $h({x};\theta)$ 是以 $\theta$ 为参数的函数，也称为模型。常见的假设空间有 [[线性模型]] 或 [[非线性模型]]。
- 学习准则：我们希望从 $\mathcal{H}$ 中找到一个合适的模型 $h({x}, \theta^{\ast})$，满足与 $y = g({x})$ 一致，即
  logseq.order-list-type:: number
  
  $$ |h({x}, \theta^{\ast}) - y| < \epsilon, \quad \forall ({x},y) \in \mathcal{X} \times \mathcal{Y}, $$
  
  或者与真实条件概率分布一致 $p_r(y|\mathbf{x})$
  
  $$ |h({x},\theta^{\ast}) - p_r(y|{x})| < \epsilon, \quad \forall ({x}, y) \in \mathcal{X} \times \mathcal{Y}, $$
  
  其中 $\epsilon$ 是个很小的正数。
	- 损失函数：损失函数 $\mathcal{L}(y, h(x;\theta)):\mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}^+$ 输入一个真实样本点 $y = g(x)$ 和逼近模型 $h(x;\theta)$ 在 $x$ 的取值，输出在 $x$ 处真实数据和预测数据的差距。常见的损失函数有 [[0-1损失函数]]、 [[平方损失函数]]、[[交叉熵损失函数]]。
	  logseq.order-list-type:: number
	- 风险最小化准则：当不固定 $\theta$ 时，$h(x;\theta)$ 模型的好坏可以通过期望风险（Expected Risk）来衡量，
	  logseq.order-list-type:: number
	  
	  $$ \mathcal{R}(\theta) = \mathbb{E}_{(x,y) \sim p(x,y)} [\mathcal{L}(y, h(x;\theta))], $$
	  
	  其中 $p(x,y)$ 是真实的数据分布，$\mathcal{L}(y,h(x;\theta))$ 为损失函数。在给定有限个数据点的情况下即真实数据集与拟合函数的整体误差，即
	     
	     $$ \mathcal{R}(\theta) = \frac{1}{M} \sum\limits_{i = 1}^M \mathcal{L}(y^{(i)}, h(x^{(i)};\theta)). $$
- 优化算法：建立模型后只需要最小化期望风险求出参数 $\theta^{\ast}$，常用的优化方法有[[梯度下降]] 、[[随机梯度下降]]、[[批量梯度下降]]、[[牛顿法]] 等。
  logseq.order-list-type:: number
-
- ## 机器学习的问题
- Approximation：即找到一个预测空间。
- Optimization：在预测空间进行优化
- Generalization：从预测模型到真实模型到底能预测到多好？
-
- ## 机器学习的类型
- [[监督学习]]：给定 $X$ 和对应的标签 $Y$，学习出 $X,Y$ 的关系。
- 无监督学习：只有数据集 $X$，没有标签 $Y$，此时可以使用 [[聚类]] 进行分类。
- 强化学习：让计算机不断尝试，试出效果最好的
- [[深度学习]]：使用神经网络解决监督学习问题。
-
-
-
-
-