-
- 目标：降低 Batch Gradient Descent 在巨大数据集情况下的开销。
- Stochastic Gradient Descent：每一步随机选取一个样本点 $i$，取该点的梯度进行下降：
	- For j = 1 to M:
		- $\theta_j := \theta_j - \alpha(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$
- 实践：大多数实际情况下会采用随机梯度下降，但是会随着学习慢慢降低 learning rate 以减小震荡。