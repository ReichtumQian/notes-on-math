-
- **定义**(批量梯度下降). 设 $f_{\theta}(x)$ 为神经网络，$\theta$ 为网络参数，每次选取 $K$ 个训练样本 $\{(x^{(k)}, y^{(k)})\}_{k = 1}^K$，第 $k$ 次迭代
  
  $$ \theta_j := \theta_j - \alpha \sum\limits_{i = 1}^M (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}, $$

  其中 $\alpha$ 是 learning rate，一般取为 $0.01$。

-
-
- 收敛性分析：往往梯度下降能收敛到最优值，$\alpha$ 越大收敛越快，但是可能越过最低点；$\alpha$ 越小收敛越慢，但是更可能收敛到最小值。
- 缺点：Batch Gradient Descent 将所有数据点都参与梯度下降，当数据量非常大时，每一步下降的计算开销很大。
