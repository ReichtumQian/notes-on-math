-
- 不断使用下面的式子进行迭代，直至收敛。其中 $\alpha$ 是 learning rate，一般取为 $0.01$。
  
  $$ \theta_j := \theta_j - \alpha \sum\limits_{i = 1}^M (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}. $$
-
-
- 收敛性分析：往往梯度下降能收敛到最优值，$\alpha$ 越大收敛越快，但是可能越过最低点；$\alpha$ 越小收敛越慢，但是更可能收敛到最小值。
- 缺点：Batch Gradient Descent 将所有数据点都参与梯度下降，当数据量非常大时，每一步下降的计算开销很大。