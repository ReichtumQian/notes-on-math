## 资料
- 李宏毅课程：[李宏毅机器学习课程主页](https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php)，作业见 [[李宏毅机器学习课程笔记]]。
-
- ## 模型问题
- 数据：共 $M$ 组数据 $\{(x^{(m)}, y^{(m)})\}_{m = 1}^M$，其中 $x^{(m)} \in \mathbb{R}^D$，即 $x^{(m)} = (x_1^{(m)},\cdots, x_D^{(m)})$。
- 模型：模型空间为 $\{f(x;\theta)\}$，$f(x;\theta):\mathbb{R}^D \rightarrow \mathbb{R}$ 其中 $\theta$ 是参数，在神经网络中即各个神经元的权重。
- 神经元：$(x_1,\cdots, x_D)$，用 $z = \sum_{d = 1}^D w_dx_d+b = \mathbf{w}^T\mathbf{x} + \mathbf{b}$ 表示输入的加权和，$f$ 是一个非线性函数（称为 Activation Function），神经元输出 Activation
  
  $$ a = f(z). $$
	- Activation Function 的选择： [[Sigmoid 函数]]、[[ReLU 函数]]
- 损失函数：
-
-
- ## 神经网络构成
- ### 神经网络基础结构
- 神经网络由多层组成，每层包括多个神经元（节点），常见的网络包括 [[前馈网络]]、[[记忆网络]]、[[图网络]]、[[循环神经网络]]、[[残差网络]]。
- ![image.png](../assets/image_1717985681141_0.png){:height 268, :width 782}
-
- ### 神经元
-
- ### 常用神经网络模型
- [[卷积神经网络 CNN]]：`CNN` 是主要用于图像处理的神经网络
-
- ## 网络优化
- 一般网络优化采用 [[随机梯度下降]]、[[批量梯度下降]] 进行优化
-
- ### Batch 批量大小的选择
- 在实际深度学习中，会将数据分为多个 Batch 计算 Gradient 并进行优化，每次计算 Batch 大小的数据再进行优化
	- 大 Batch：计算时间较长，每次优化较稳定，但容易被卡住。可以通过 GPU 并行计算增加运行效率。
	- 小 Batch：每次计算时间较短，每次优化可能方向会变动较大，但是不太容易被卡住
- ### Learning rate 学习率的选择
-
-
-
-
-
-
- ## 常见训练问题
- ![image.png](../assets/image_1719279869848_0.png){:height 363, :width 452}
- 如果在训练数据上 Loss 很大：可能是
	- 模型弹性不足：可以【增加更多 Feature】或者【增加网络深度】
	- 优化效果不好：如果梯度太小，则可能是【局部极值点】或者【鞍点】
		- 如何判断是不是鞍点：根据 [[Hessian 矩阵]] 判断是否为鞍点，若 $H$ 正定，则为局部极小值；若 $H$ 负定，则为局部最大值；否则为鞍点。
		- 如果是极值点则没救了。如果是鞍点，可以朝着 $H$ 负特征值对应的特征向量方向继续优化。
	- 如何判断模型弹性是否足够：
		- 先测试最简单的模型，将后续复杂模型的 Loss 与简单模型对比，如果 Loss 反而大就是优化没做好。
		- 通过测试不同弹性的模型，例如增加层数，看看效果能不能变好。
- 如果训练数据上 Loss 小，但是测试数据上 Loss 大，则为 Overfitting（类似于 Runge 现象）。
	- 解决方案1：增加数据量
	- 解决方案2：限制模型弹性，例如【使用更加专用的神经网络】、【交叉验证】。
-
-
-
