-
- **定义**(线性回归). 共 $M$ 个样本为 $(x^{(1)}, y^{(1)}),\cdots, (x^{(M)}, y^{(M)})$。每个样本有 $n$ 个特征，记为 $x_1,\cdots, x_n$，例如第一个样本的属性为 $x^{(1)}_1,\cdots,x^{(M)}_n$，线性回归根据样本点生成函数
  
  $$ h(x) = \sum\limits_{i = 0}^n \theta_i x_i, $$
  
  其中 $x_0 = 1$ 为虚拟特征。
- 考虑最小化二次误差
  
  $$ \min\limits_{\theta} J(\theta) := \min \limits_{\theta} \sum\limits_{i = 1}^M (h_{\theta}(x^{(i)}) - y^{(i)})^2. $$
-
- ## Batch Gradient Descent 整体梯度下降
- 考虑使用 [[梯度下降]] 求解出 $\theta$ ：不断使用下面的式子进行迭代，直至收敛。其中 $\alpha$ 是 learning rate，一般取为 $0.01$。
  
  $$ \theta_j := \theta_j - \alpha \sum\limits_{i = 1}^M (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}. $$
- 收敛性分析：往往梯度下降能收敛到最优值，$\alpha$ 越大收敛越快，但是可能越过最低点；$\alpha$ 越小收敛越慢，但是更可能收敛到最小值。
- 缺点：Batch Gradient Descent 将所有数据点都参与梯度下降，当数据量非常大时，每一步下降的计算开销很大。
-
- ## Stochastic Gradient Descent 随机梯度下降
-
-
-
