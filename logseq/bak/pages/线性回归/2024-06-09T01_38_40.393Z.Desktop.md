-
- **定义**(线性回归). 共 $M$ 个样本为 $(x^{(1)}, y^{(1)}),\cdots, (x^{(M)}, y^{(M)})$。每个样本有 $n$ 个特征，记为 $x_1,\cdots, x_n$，例如第一个样本的属性为 $x^{(1)}_1,\cdots,x^{(M)}_n$，线性回归根据样本点生成函数
  
  $$ h(x) = \sum\limits_{i = 0}^n \theta_i x_i, $$
  
  其中 $x_0 = 1$ 为虚拟特征，方便起见也会写为 $h(x) = \theta^T x$。
- 如何求解：考虑最小化二次误差
  
  $$ \min\limits_{\theta} J(\theta) := \min \limits_{\theta} \sum\limits_{i = 1}^M (h_{\theta}(x^{(i)}) - y^{(i)})^2. $$
- 求解方法：[[梯度下降]] 求出 $\theta$。
- ## 为什么用平方误差？
- 线性回归求解中往往都使用平方误差，本节分析其用意。假设有 $M$ 个数据集 $(x^{(1)}, y^{(1)}), \cdots, (x^{(M)}, y^{(M)})$，其中 $x^{(i)} \in \mathbb{R}^n, y^{(i)} \in \mathbb{R}$，假设数据集会有误差
  
  $$ y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)},$$
  
  其中 $\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^2)$ 服从 [[正态分布]]，此时满足 $x^{(i)}$ 条件下 $y^{(i)}$ 出现的概率为 $y^{(i)}|x^{(i)} \sim \mathcal{N}(\theta^T x^{(i)}, \sigma^2)$，其中 $\theta$ 为参数（选择正态分布的原因是 [[Lindeberg-Levy 中心极限定理]]）。
- 考虑 [[似然函数]] $L(\theta; X, \mathbf{y}) = p(\mathbf{y} |X; \theta)$，其中 $X = [(x^{(1)})^T, \cdots, (x^{(M)})^T]^T$，$\mathbf{y} = [y^{(1)},\cdots, y^{(M)}]^T$，则
  
  $$
  \begin{align*}
  L(\theta) &= \prod \limits_{i = 1}^M p(y^{(i)}|x^{(i)}; \theta)\\
  &= \prod \limits_{i = 1}^n \frac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2} \right)
  \end{align*}
  $$
  
  似然函数表达着各个 $\theta$ 参数取值情况下观测数据的合理性，我们希望最大化似然函数，不妨对似然函数取对数
  
  $$
  \begin{align*}
  \ell(\theta) &= \log L(\theta) = n \log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2} \sum\limits_{i = 1}^M (y^{(i)} - \theta^T x^{(i)})^2.
  \end{align*}
  $$
  
  因此要最大化似然函数即最小化 $\frac{1}{2} \sum\limits_{i = 1}^n (y^{(i)} - \theta^T x^{(i)})^2$。
- ## Batch Gradient Descent 整体梯度下降
- Batch Gradient Descent：不断使用下面的式子进行迭代，直至收敛。其中 $\alpha$ 是 learning rate，一般取为 $0.01$。
  
  $$ \theta_j := \theta_j - \alpha \sum\limits_{i = 1}^M (h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}. $$
- 收敛性分析：往往梯度下降能收敛到最优值，$\alpha$ 越大收敛越快，但是可能越过最低点；$\alpha$ 越小收敛越慢，但是更可能收敛到最小值。
- 缺点：Batch Gradient Descent 将所有数据点都参与梯度下降，当数据量非常大时，每一步下降的计算开销很大。
-
- ## Stochastic Gradient Descent 随机梯度下降
- 目标：降低 Batch Gradient Descent 在巨大数据集情况下的开销。
- Stochastic Gradient Descent：每一步随机选取一个样本点 $i$，取该点的梯度进行下降：
	- For j = 1 to M:
		- $\theta_j := \theta_j - \alpha(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$
- 实践：大多数实际情况下会采用随机梯度下降，但是会随着学习慢慢降低 learning rate 以减小震荡。
-
- ## Normal Equation
- Normal Equation：即最小二乘算法中的 Normal Equation。
-
-
-