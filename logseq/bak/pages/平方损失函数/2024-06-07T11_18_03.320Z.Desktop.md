-
- 如何求解：考虑最小化二次误差
  $$ \min\limits_{\theta} J(\theta) := \min \limits_{\theta} \sum\limits_{i = 1}^M (h_{\theta}(x^{(i)}) - y^{(i)})^2. $$
-
-
- ## 为什么用平方误差？
- 线性回归求解中往往都使用平方误差，本节分析其用意。假设有 $M$ 个数据集 $(x^{(1)}, y^{(1)}), \cdots, (x^{(M)}, y^{(M)})$，其中 $x^{(i)} \in \mathbb{R}^n, y^{(i)} \in \mathbb{R}$，假设数据集会有误差
  
  $$ y^{(i)} = \theta^T x^{(i)} + \epsilon^{(i)},$$
  
  其中 $\epsilon^{(i)} \sim \mathcal{N}(0, \sigma^2)$ 服从 [[正态分布]]，此时满足 $x^{(i)}$ 条件下 $y^{(i)}$ 出现的概率为 $y^{(i)}|x^{(i)} \sim \mathcal{N}(\theta^T x^{(i)}, \sigma^2)$，其中 $\theta$ 为参数（选择正态分布的原因是 [[Lindeberg-Levy 中心极限定理]]）。
- 考虑 [[似然函数]] $L(\theta; X, \mathbf{y}) = p(\mathbf{y} |X; \theta)$，其中 $X = [(x^{(1)})^T, \cdots, (x^{(M)})^T]^T$，$\mathbf{y} = [y^{(1)},\cdots, y^{(M)}]^T$，则
  
  $$
  \begin{align*}
  L(\theta) &= \prod \limits_{i = 1}^M p(y^{(i)}|x^{(i)}; \theta)\\
  &= \prod \limits_{i = 1}^n \frac{1}{\sqrt{2\pi}\sigma} \mathrm{exp}\left( - \frac{(y^{(i)} - \theta^T x^{(i)})^2}{2 \sigma^2} \right)
  \end{align*}
  $$
  
  似然函数表达着各个 $\theta$ 参数取值情况下观测数据的合理性，我们希望最大化似然函数，不妨对似然函数取对数
  
  $$
  \begin{align*}
  \ell(\theta) &= \log L(\theta) = n \log \frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{\sigma^2} \cdot \frac{1}{2} \sum\limits_{i = 1}^M (y^{(i)} - \theta^T x^{(i)})^2.
  \end{align*}
  $$
  
  因此要最大化似然函数即最小化 $\frac{1}{2} \sum\limits_{i = 1}^n (y^{(i)} - \theta^T x^{(i)})^2$。
-
-
-